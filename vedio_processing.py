# -*- coding: utf-8 -*-
"""vedio_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNbdyOL97doCIeanhL2xnfspruznD0eF

# Instal libraries
"""

!pip install opencv-python torch torchvision deep_sort_realtime
!pip install yt_dlp
!pip install yt_dlp opencv-python mtcnn deep_sort_realtime torchvision
!pip install facenet-pytorch
!pip install -q ultralytics deep_sort_realtime
!pip install ultralytics

"""# Compress Video"""

import yt_dlp

video_url = "https://youtu.be/s0DNQig0MGE?si=shp5nVwcsfOhIzzp"
output_file = "video.mp4"

ydl_opts = {'format': 'best', 'outtmpl': output_file}
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download([video_url])

# Compress video
!ffmpeg -i video.mp4 -vf scale=640:360 compressed_video.mp4 -y

"""# Load the model"""

import torch
from torchvision import models

model = models.resnet18(weights=None)
model.fc = torch.nn.Linear(model.fc.in_features, 7)
model.load_state_dict(torch.load('/content/emotion_model.pth', map_location='cpu'))  # <-- غيّر المسار إذا لزم
model.eval()

emotion_map = ["Stress", "Anger", "Frustration", "Happiness", "Surprise", "Focus", "Neutral"]

"""# Generate emotional and physical info for players"""

import cv2, torch, json, os
from PIL import Image
from collections import defaultdict
from torchvision import models, transforms
from deep_sort_realtime.deepsort_tracker import DeepSort
from facenet_pytorch import MTCNN


labels = ["Stress","Anger","Frustration","Happiness","Surprise","Focus","Neutral"]
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


mtcnn = MTCNN(select_largest=False, device=device)
tracker = DeepSort(max_age=30)
os.makedirs("players", exist_ok=True)


video_path = "/content/compressed_video.mp4"
cap = cv2.VideoCapture(video_path)


data = defaultdict(lambda: {"emotion_readings": [], "physical_readings": []})
saved_ids = set()
previous_positions = {}

frame_idx = 0
sample_rate = 5

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break
    frame_idx += 1
    if frame_idx % sample_rate != 0: continue

    boxes, _ = mtcnn.detect(frame)
    detections = []
    if boxes is not None:
        for (x1, y1, x2, y2) in boxes:
            x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))
            w, h = x2 - x1, y2 - y1
            if w < 40 or h < 40: continue
            detections.append(([x1, y1, w, h], 0.9, "face"))

    tracks = tracker.update_tracks(detections, frame=frame)
    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0

    for tr in tracks:
        if not tr.is_confirmed(): continue
        tid = tr.track_id
        l, t, r, b = map(int, tr.to_ltrb())
        face = frame[t:b, l:r]
        if face.size == 0: continue

        # Save player photo
        if tid not in saved_ids:
            cv2.imwrite(f"players/{tid}.jpg", face)
            saved_ids.add(tid)

        # Classifiy emotion
        img = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
        tt = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            model_output = model(tt)
            # Ensure model_output is a tensor before applying argmax
            if isinstance(model_output, torch.Tensor):
                emo = labels[torch.argmax(model_output, axis=1).item()]
            else:
                # Handle the case where model output is not a tensor
                print(f"Warning: Model output for track {tid} at time {timestamp:.2f}s is not a tensor: {type(model_output)}")
                emo = "Unknown" # Or handle as appropriate


        data[tid]["emotion_readings"].append({
            "time": f"{timestamp:.2f}",
            "emotion": emo
        })

        # Physiacl info
        center_x = int((l + r) / 2)
        center_y = int((t + b) / 2)
        speed = 0
        direction = "Unknown"

        if tid in previous_positions:
            dx = center_x - previous_positions[tid][0]
            dy = center_y - previous_positions[tid][1]
            speed = round((dx**2 + dy**2)**0.5, 2)
            if abs(dx) > abs(dy):
                direction = "Right" if dx > 0 else "Left"
            else:
                direction = "Down" if dy > 0 else "Up"

        previous_positions[tid] = (center_x, center_y)

        data[tid]["physical_readings"].append({
            "time": f"{timestamp:.2f}",
            "x": center_x,
            "y": center_y,
            "speed": speed,
            "direction": direction
        })

        print(f"[T{tid}] {emo} @ {timestamp:.2f}s | Pos: ({center_x},{center_y}) | Speed: {speed} | Dir: {direction}")

cap.release()

# Save report
with open("player_report.json", "w") as f:
    json.dump(data, f, indent=2)

print(f"Done  {len(data)}")

"""# Convert ids to players names"""

import os
import json
from PIL import Image
import matplotlib.pyplot as plt

players_dir = "/content/players"
output_json = "player_names.json"
player_names = {}


if not os.path.exists(players_dir):
    print(" not exist ")
else:
    image_files = sorted(os.listdir(players_dir))

    for img_file in image_files:
        if not img_file.endswith(".jpg"):
            continue

        track_id = os.path.splitext(img_file)[0]
        img_path = os.path.join(players_dir, img_file)


        img = Image.open(img_path)
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"Track ID: {track_id}")
        plt.show()


        name = input(f" Enter plyer for Track ID {track_id}: ").strip()
        if name:
            player_names[track_id] = name

    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(player_names, f, ensure_ascii=False, indent=2)

    print(f"Done {output_json}")

"""# Combine the player report with names report"""

import json


emotion_file = "/content/player_report.json"
names_file = "/content/player_names.json"
output_file = "Final_report_named.json"

with open(emotion_file, "r", encoding="utf-8") as f:
    emotion_data = json.load(f)

with open(names_file, "r", encoding="utf-8") as f:
    name_data = json.load(f)


named_report = {}

for track_id, emotions in emotion_data.items():
    name = name_data.get(track_id, f"Unknown_{track_id}")
    named_report[name] = emotions


with open(output_file, "w", encoding="utf-8") as f:
    json.dump(named_report, f, ensure_ascii=False, indent=2)

print(f" Done {output_file}")

"""After running the above two cells, please go to "Runtime" -> "Restart runtime" and then run your code cells from the beginning. This will ensure the new installations are active."""