# -*- coding: utf-8 -*-
"""LLM_feedback.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VPmstg0KDLV_y3a5hssfpG473SWWkY7y

# LLM feedback for each player
"""

!pip install transformers accelerate huggingface_hub

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø®ÙÙŠÙ Ù…Ù† Ù‡Ø§Ø¬ÙŠÙ†Øº ÙÙŠØ³ (ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… Ù…ÙˆØ¯ÙŠÙ„ Ø¢Ø®Ø±)
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù„ÙƒÙ„ Ù„Ø§Ø¹Ø¨
with open('/content/Final_report_named.json', 'r') as f:
    emotion_data = json.load(f)

# ØªÙˆÙ„ÙŠØ¯ ØªÙ‚ÙŠÙŠÙ… Ù†ÙØ³ÙŠ Ù„ÙƒÙ„ Ù„Ø§Ø¹Ø¨
player_feedbacks = {}

for player_name, data in emotion_data.items():
    readings = data.get("emotion_readings", [])

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØªØ­Ø¬ÙŠÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ù„ØªÙƒÙˆÙ† Ù…ØªÙ†ÙˆØ¹Ø©
    seen = set()
    filtered = []
    for r in readings:
        key = (r["time"], r["emotion"])
        if key not in seen:
            seen.add(key)
            filtered.append(r)
        if len(filtered) >= 20:
            break

    # ØªÙˆÙ„ÙŠØ¯ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
    sequence = ", ".join([f"{r['time']}s: {r['emotion']}" for r in filtered]) or "No emotional data available"

    # Ø¨Ø±ÙˆÙ…Ø¨Øª ÙˆØ§Ø¶Ø­ Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ
    prompt = (
        f"You are a professional sports psychologist. Based on the emotional timeline of player '{player_name}', "
        "write a meaningful psychological report that describes their emotional state, behavioral patterns, and mental resilience.\n\n"
        f"Emotional observations:\n{sequence}\n\n"
        "Psychological evaluation:"
    )

    print(f"\nGenerating feedback for {player_name}...")

    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(model.device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=250,
            do_sample=True,
            top_p=0.95,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id
        )
        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        evaluation = full_output.replace(prompt, "").strip()

        player_feedbacks[player_name] = evaluation or "No meaningful output generated."
    except Exception as e:
        player_feedbacks[player_name] = f"[ERROR] Failed to generate: {str(e)}"

# Ø·Ø¨Ø§Ø¹Ø© ÙƒÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
print("\nðŸ§  Psychological Evaluations:\n")
for name, feedback in player_feedbacks.items():
    print(f"--- Evaluation for {name} ---")
    print(feedback)
    print("-" * 40)